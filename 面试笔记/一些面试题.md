### 一、HashMap&ConcurrentHashMap

- 底层数据结构为hash表，1.7扩容时由于头插法会产生循环链表，导致线程不安全  ，此外 1.7 1.8都会有数据覆盖的问题
  - hashmap继承AbstractMap 支持null的key
  - hashtable是线程安全的 但是效率低，它把每个方法都加了synchronized ，继承Dictionary,不支持null的key、value
  - 而concurrentHashMap效率较高，不允许null的key和value
- 1.8将较长的链表改为红黑树，触发扩容的操作
  - 哈希table为 null或者长度为0
  - 键值对数量超出阈值threshold
  - 链表的长度超出一个阈值但是低于转为红黑树的阈值



### 二、 乐观锁、悲观锁

1. 乐观锁：在操作数据时非常乐观，认为别人不会同时修改数据。因此乐观锁不会上锁，只是在更新的时候判断一下在此期间别人是否修改了数据，如果别人修改了 就放弃操作。两种实现机制
   1. CAS（compare And Swap）内存位置，比较预期值，拟写入的新值
   2. 版本号机制
2. 悲观锁：在操作数据时比较悲观，认为别人会同时修改数据。因此操作数据时把数据所著，直到完成后才释放锁



题量、审题、 重复

审题、 关注测试、 由少而多

手动事务批处理 bentch  50、100

内存模型、jvm、gc、springboot框架 



内存模型：堆、栈、本地方法栈、方法区（静态区）、程序计数器

 堆和方法区是线程共享的

类加载器：启动类加载器（BootStrap）javahome/lib、扩展类加载器runtime/lib/ext、系统类加载器classpath。双亲委派机制

gc机制和算法：

​	堆中内存非为新生代、老年代和永久代，其中新生代又分为Eden（所有新建的对象都在eden 伊甸园）、（Survivor）s1 from区、（Survivor） s2 to区

<img src="C:\Users\hanka\AppData\Roaming\Typora\typora-user-images\image-20210304160508231.png" alt="image-20210304160508231" style="zoom:50%;" />

GC分类：

​	1、新生代垃圾回收器：新生代minor GC  、老年代 majorGC。

​	复制清除算法执行minorGC：当需要GC时，将EDen和s1还存活的对象复制到s2去，然后清楚eden和s1，下次gc就把eden和s2复制到s1中。如果对象的存活率较高，s1/s2内存空间不够时依赖老年代进行分配担保、

​	2、老年代

​	jvm使用分代收集的思想来管理内存，即对不同的区域实行不同的回收策略，jvm为每一个对象定义了一个年龄计数器，每gc中存活一次+1，默认15次就到了老年代。

​	当老年代空间不足时，触发majorGC，主副GC都会触发stop the world，停止大部分线程，单副gc会快，还有一种fullGC新生代老年代都GC

线程池的核心参数和基本原理：

​	1、核心线程数、最大线程数、缓冲队列数、缓冲队列满了才会申请超过核心线程数的线程

redis数据类型：string、hash、list、set、sorted set  

读未提交、读已提交、可重复读、串行化

### 三、Redis 为什么这么快？除了基于内存操作还有其他原因吗？

string hash list set sortedSet  k-v数据库

采用标准c写的，单进程单线程 不用cpu状态切换、上下文切换，也不需要考虑各种锁的问题，redis的瓶颈是机器内存和网络带宽

非阻塞io：redis采用eoll作为IO多路复用技术的实现，再加上redis自身的事件处理模型将epoll中的连接读写关闭

1. 持久化采用Copy-On-Write（写时复制）技术，异步保存在磁盘上，主要有两种策略：

>核心思路：fork一个子进程 只有在父进程发生写操作修改内存数据时，才去分配内存空间并只需要复制被修改的内存页的数据并不是全部数据
>
>

1. 一是根据时间更新快照（在指定时间间隔对数据进行快 照存储） 主进程fork一个子进程进行持久化
2.  二是基于语句追加方式（Append-only file aof）记录每次对服务器写的操作，当服务器重启的时候重新执行这些命令来恢复原始数据：有always：每个命令都立即同步到aof，很慢但是很安全；everysec每秒同步一次是折中的方案，no redis不处理交给os来处理，非常快但是最不安全。
3. 恢复时先恢复aof文件

### 四、慢查询解决过程

>慢sql特征
>
>1、数据库cpu负载高，查询语句中有很多计算逻辑，导致数据库cpu负载
>
>2、IO负载高导致服务器卡住 一般和全表查询没索引有关系
>
>3、查询语句正常，索引正常但还是慢，如果表面索引正常，但是查询慢，需要查看索引是否生效

开启慢查询日志

场景一：join匹配几十万条站址记录，对每一个站（每一条数据需要查询一个百万级别的数据库，统计其和并填入本行），并且要求重复性检验 约（七八个小时）

场景二：一个月产生的百万条excel记录需要导入单表中，并且要求重复性检验（两三个小时）

>  总结问题：速度慢的过程都是出现在了大量数据的重复性检验并插入
>
> - 查询数据库的校验对每一行数据都要查询一次数据库，应用访问数据库来回的网络IO次数被放大了 n 倍，时间也就放大了 n 倍
> - 写入数据也是逐行写入的，问题和上面的一样
> - 频繁的提交事务，

解决方案：

​	开始以为是没用使用到索引

​	1.重复性检验时，避免一条一条的检验，而是将数据库中数据一次性取出放入hashmap中缓存起来，判断哪些是需要更新的

	2. 插入和更新时避免一条一条的插入，通过mybatis foreach结合java集合实现批量操作，一次性操作数量太多也会出现性能限制，太多会触发mysql的缓存设置大小而使用swap分区，降低速度，这里选择一千条拼接一次。除此之外再开启多个线程进行导入。
 	3. mybatis开启事务批处理，多条sql一起提交；
 	4. 读excel也不是用poi而使用网上查的easyExcel，速度更快



### 五、rabbitMQ相关





​	